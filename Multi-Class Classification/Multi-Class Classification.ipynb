{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13744be5",
   "metadata": {},
   "source": [
    "# Multi-Class Classification\n",
    "Exploration of multi-class classifiaction through the classic MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a5d245",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "  \n",
    "This MNIST dataset contains a lot of examples:\n",
    "\n",
    "* The MNIST training set contains 60,000 examples.\n",
    "* The MNIST test set contains 10,000 examples.\n",
    "\n",
    "Each example contains a pixel map showing how a person wrote a digit. For example, the following images shows how a person wrote the digit `1` and how that digit might be represented in a 14x14 pixel map (after the input data is normalized). \n",
    "\n",
    "![Two images. The first image shows a somewhat fuzzy digit one. The second image shows a 14x14 floating-point array in which most of the cells contain 0 but a few cells contain values between 0.0 and 1.0. The pattern of nonzero values corresponds to the image of the fuzzy digit in the first image.](https://www.tensorflow.org/images/MNIST-Matrix.png)\n",
    "\n",
    "Each example in the MNIST dataset consists of:\n",
    "\n",
    "* A label specified by a [rater](https://developers.google.com/machine-learning/glossary/#rater).  Each label must be an integer from 0 to 9.  For example, in the preceding image, the rater would almost certainly assign the label `1` to the example.\n",
    "* A 28x28 pixel map, where each pixel is an integer between 0 and 255. The pixel values are on a gray scale in which 0 represents white, 255 represents black, and values between 0 and 255 represent various shades of gray.  \n",
    "\n",
    "This is a multi-class classification problem with 10 output classes, one for each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "839e318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import relevant modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "# The following line improves formatting when ouputting NumPy arrays.\n",
    "np.set_printoptions(linewidth = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7e54d",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "`tf.keras` provides a set of convenience functions for loading well-known datasets. Each of these convenience functions does the following:\n",
    "* Loads both the training set and the test set.\n",
    "* Separates each set into features and labels.\n",
    "\n",
    "The relevant convenience function for MNIST is called `mnist.load_data()`\n",
    "\n",
    "Notice that `mnist.load_data()` returned four separate values:\n",
    "\n",
    "* `x_train` contains the training set's features.\n",
    "* `y_train` contains the training set's labels.\n",
    "* `x_test` contains the test set's features.\n",
    "* `y_test` contains the test set's labels.\n",
    "\n",
    "**Note:** The MNIST .csv training set is already shuffled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c982f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b584dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   9, 163, 144,  39,  24,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 239, 254, 254, 186,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 112, 149, 250, 254, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  25, 207, 244,  67,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  99, 254, 189,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3, 151, 254, 212,  19,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81, 254, 254, 188,  27,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   8, 198, 254, 254, 167,  10,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   2,  17, 103, 229, 254, 254,  77,   7,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  56, 169, 254, 254, 254, 254, 123,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  88, 254, 254, 254, 254, 207,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  43, 177, 233, 250, 254, 195,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 155, 254, 209,  12,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 201, 254, 254,  48,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6, 207, 254, 222,  23,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  84, 138,   0,   0,   5, 106, 254, 254, 167,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 216, 179,   0,   0, 151, 254, 254, 192,  15,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  44, 254, 254, 183, 225, 254, 254, 254,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  32, 237, 254, 254, 254, 220, 184,  99,   5,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  28, 139, 254, 171,  75,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[2132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8047ab86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x200ae35cc50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[213])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3349870a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x200b4918f50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature values normalization:\n",
    "x_train_normalized = x_train / x_train.std()\n",
    "x_test_normalized = x_test / x_train.std()\n",
    "plt.imshow(x_train_normalized[213])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4a35ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the plot_curve function.\n"
     ]
    }
   ],
   "source": [
    "#@title Define the plotting function\n",
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "  # list_of_metrics should be one of the names shown in:\n",
    "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Value\")\n",
    "\n",
    "  for m in list_of_metrics:\n",
    "    x = hist[m]\n",
    "    plt.plot(epochs[1:], x[1:], label=m)\n",
    "\n",
    "  plt.legend()\n",
    "\n",
    "print(\"Loaded the plot_curve function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b296c",
   "metadata": {},
   "source": [
    "## Create a deep neural net model\n",
    "The `create_model` function defines the topography of the deep neural net, specifying the following:\n",
    "* The number of layers in the deep neural net.\n",
    "* The number of nodes in each layer.\n",
    "* Any refularization layers.\n",
    "\n",
    "The `create_model` function also defines the activation function of each layer. The activation function of the output layer is softmax, which will yield 10 different outputs for each example.\n",
    "Each of the 10 outputs provides the probability that the input example is a certain digit.\n",
    "\n",
    "**Note**: This exercise does not define feature columns or a feature layer: we will train the model on a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "226a1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(my_learning_rate):\n",
    "    \"\"\"\n",
    "    Create and compile a deep neural net.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Features are stored in a two dimensional 28 x 28 array.\n",
    "    # Fltten that two dimensional array into a one-dimensional 784-element array.\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "    \n",
    "    # First hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
    "    \n",
    "    # First hidden layer.\n",
    "    model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "    \n",
    "    # Define a dropout regularization layer.\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "    \n",
    "    # Define the output layer. units is 10 because the model must choose between the digits 0 to 9, inclusive.\n",
    "    model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
    "    \n",
    "    # Compile: loss is sparse_categorical_crossentropy\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate),\n",
    "                 loss=\"sparse_categorical_crossentropy\",\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_features, train_label, epochs, batch_size=None, validation_split=0.1):\n",
    "    \"\"\"\n",
    "    Train the model by feeding it the data.\n",
    "    \"\"\"\n",
    "    history = model.fit(x=train_features, y=train_label, batch_size=batch_size, epochs= epochs, shuffle=True, validation_split=validation_split)\n",
    "    \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    \n",
    "    return epochs, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df97fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 1s 201ms/step - loss: 3.2412 - accuracy: 0.0981 - val_loss: 3.2557 - val_accuracy: 0.1040\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 3.0032 - accuracy: 0.0976 - val_loss: 2.4348 - val_accuracy: 0.1010\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 2.4186 - accuracy: 0.1026 - val_loss: 2.3471 - val_accuracy: 0.1110\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 2.3454 - accuracy: 0.1116 - val_loss: 2.3211 - val_accuracy: 0.1085\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 2.3220 - accuracy: 0.1005 - val_loss: 2.3193 - val_accuracy: 0.0950\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.3108 - accuracy: 0.1071 - val_loss: 2.3110 - val_accuracy: 0.0940\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 2.3020 - accuracy: 0.1100 - val_loss: 2.3035 - val_accuracy: 0.0900\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 2.2969 - accuracy: 0.1141 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 2.2964 - accuracy: 0.1141 - val_loss: 2.3032 - val_accuracy: 0.0970\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 2.2961 - accuracy: 0.1177 - val_loss: 2.3027 - val_accuracy: 0.0970\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 2.2940 - accuracy: 0.1187 - val_loss: 2.3035 - val_accuracy: 0.0955\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 2.2937 - accuracy: 0.1239 - val_loss: 2.3041 - val_accuracy: 0.0960\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.2915 - accuracy: 0.1254 - val_loss: 2.3048 - val_accuracy: 0.0975\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 2.2893 - accuracy: 0.1280 - val_loss: 2.3064 - val_accuracy: 0.0980\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 2.2878 - accuracy: 0.1294 - val_loss: 2.3063 - val_accuracy: 0.1070\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 2.2861 - accuracy: 0.1304 - val_loss: 2.3056 - val_accuracy: 0.1060\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 2.2854 - accuracy: 0.1309 - val_loss: 2.3055 - val_accuracy: 0.1035\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 2.2817 - accuracy: 0.1364 - val_loss: 2.3071 - val_accuracy: 0.1060\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.2783 - accuracy: 0.1370 - val_loss: 2.3102 - val_accuracy: 0.1080\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 2.2780 - accuracy: 0.1374 - val_loss: 2.3095 - val_accuracy: 0.1095\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.2736 - accuracy: 0.1416 - val_loss: 2.3086 - val_accuracy: 0.1075\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 2.2750 - accuracy: 0.1434 - val_loss: 2.3097 - val_accuracy: 0.0995\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.2697 - accuracy: 0.1475 - val_loss: 2.3107 - val_accuracy: 0.1020\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.2670 - accuracy: 0.1456 - val_loss: 2.3117 - val_accuracy: 0.1035\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 2.2632 - accuracy: 0.1505 - val_loss: 2.3144 - val_accuracy: 0.1035\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.2605 - accuracy: 0.1489 - val_loss: 2.3140 - val_accuracy: 0.0985\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.2563 - accuracy: 0.1507 - val_loss: 2.3148 - val_accuracy: 0.1015\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 2.2527 - accuracy: 0.1581 - val_loss: 2.3181 - val_accuracy: 0.0945\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 2.2496 - accuracy: 0.1622 - val_loss: 2.3214 - val_accuracy: 0.1000\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 2.2427 - accuracy: 0.1653 - val_loss: 2.3225 - val_accuracy: 0.1030\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.2413 - accuracy: 0.1656 - val_loss: 2.3236 - val_accuracy: 0.1045\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.2355 - accuracy: 0.1667 - val_loss: 2.3264 - val_accuracy: 0.1000\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.2296 - accuracy: 0.1713 - val_loss: 2.3280 - val_accuracy: 0.0990\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 2.2270 - accuracy: 0.1734 - val_loss: 2.3351 - val_accuracy: 0.1030\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2.2227 - accuracy: 0.1737 - val_loss: 2.3353 - val_accuracy: 0.1030\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 2.2153 - accuracy: 0.1838 - val_loss: 2.3409 - val_accuracy: 0.1015\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.2117 - accuracy: 0.1846 - val_loss: 2.3454 - val_accuracy: 0.0975\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.2069 - accuracy: 0.1877 - val_loss: 2.3439 - val_accuracy: 0.1030\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.2022 - accuracy: 0.1889 - val_loss: 2.3482 - val_accuracy: 0.0985\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1931 - accuracy: 0.1941 - val_loss: 2.3520 - val_accuracy: 0.0990\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 2.1870 - accuracy: 0.2020 - val_loss: 2.3604 - val_accuracy: 0.0950\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 2.1797 - accuracy: 0.2026 - val_loss: 2.3607 - val_accuracy: 0.0940\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 2.1748 - accuracy: 0.2106 - val_loss: 2.3618 - val_accuracy: 0.0975\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 2.1661 - accuracy: 0.2154 - val_loss: 2.3704 - val_accuracy: 0.0980\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 2.1589 - accuracy: 0.2156 - val_loss: 2.3718 - val_accuracy: 0.1015\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 2.1534 - accuracy: 0.2144 - val_loss: 2.3813 - val_accuracy: 0.0985\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 2.1473 - accuracy: 0.2154 - val_loss: 2.3828 - val_accuracy: 0.1020\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 2.1333 - accuracy: 0.2236 - val_loss: 2.3896 - val_accuracy: 0.1025\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.1281 - accuracy: 0.2246 - val_loss: 2.3977 - val_accuracy: 0.0955\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.1144 - accuracy: 0.2401 - val_loss: 2.3995 - val_accuracy: 0.1015\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.1060 - accuracy: 0.2385 - val_loss: 2.4133 - val_accuracy: 0.0980\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 2.0977 - accuracy: 0.2405 - val_loss: 2.4122 - val_accuracy: 0.0975\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 2.0914 - accuracy: 0.2428 - val_loss: 2.4197 - val_accuracy: 0.1005\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.0922 - accuracy: 0.2419 - val_loss: 2.4313 - val_accuracy: 0.0975\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 2.0703 - accuracy: 0.2559 - val_loss: 2.4418 - val_accuracy: 0.0990\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 2.0662 - accuracy: 0.2511 - val_loss: 2.4463 - val_accuracy: 0.1005\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 2.0573 - accuracy: 0.2599 - val_loss: 2.4404 - val_accuracy: 0.0960\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.0477 - accuracy: 0.2576 - val_loss: 2.4753 - val_accuracy: 0.0980\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0437 - accuracy: 0.2604 - val_loss: 2.4698 - val_accuracy: 0.1015\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 2.0300 - accuracy: 0.2671 - val_loss: 2.4670 - val_accuracy: 0.0995\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 2.0214 - accuracy: 0.2751 - val_loss: 2.4978 - val_accuracy: 0.0975\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 2.0102 - accuracy: 0.2736 - val_loss: 2.4767 - val_accuracy: 0.1010\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 1.9966 - accuracy: 0.2850 - val_loss: 2.5055 - val_accuracy: 0.0965\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 1.9831 - accuracy: 0.2797 - val_loss: 2.5106 - val_accuracy: 0.1035\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.9834 - accuracy: 0.2860 - val_loss: 2.5286 - val_accuracy: 0.1000\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.9683 - accuracy: 0.2889 - val_loss: 2.5244 - val_accuracy: 0.0995\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.9586 - accuracy: 0.2969 - val_loss: 2.5514 - val_accuracy: 0.1015\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.9484 - accuracy: 0.2999 - val_loss: 2.5399 - val_accuracy: 0.1025\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.9455 - accuracy: 0.3020 - val_loss: 2.5763 - val_accuracy: 0.0925\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.9306 - accuracy: 0.3002 - val_loss: 2.5647 - val_accuracy: 0.0995\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.9157 - accuracy: 0.3203 - val_loss: 2.5827 - val_accuracy: 0.1010\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.9150 - accuracy: 0.3131 - val_loss: 2.6017 - val_accuracy: 0.0970\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.9021 - accuracy: 0.3212 - val_loss: 2.5840 - val_accuracy: 0.1020\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 1.8910 - accuracy: 0.3244 - val_loss: 2.6205 - val_accuracy: 0.1045\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.8705 - accuracy: 0.3281 - val_loss: 2.6554 - val_accuracy: 0.1015\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 1.8670 - accuracy: 0.3275 - val_loss: 2.6346 - val_accuracy: 0.1005\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.8622 - accuracy: 0.3311 - val_loss: 2.6548 - val_accuracy: 0.1000\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.8573 - accuracy: 0.3346 - val_loss: 2.6706 - val_accuracy: 0.1030\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.8488 - accuracy: 0.3354 - val_loss: 2.6531 - val_accuracy: 0.1030\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.8358 - accuracy: 0.3385 - val_loss: 2.6961 - val_accuracy: 0.0920\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.8131 - accuracy: 0.3492 - val_loss: 2.7054 - val_accuracy: 0.0995\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.8050 - accuracy: 0.3587 - val_loss: 2.7227 - val_accuracy: 0.0920\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.7818 - accuracy: 0.3686 - val_loss: 2.7186 - val_accuracy: 0.1000\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.7802 - accuracy: 0.3697 - val_loss: 2.7698 - val_accuracy: 0.1045\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.7826 - accuracy: 0.3650 - val_loss: 2.7636 - val_accuracy: 0.0995\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.7639 - accuracy: 0.3713 - val_loss: 2.7238 - val_accuracy: 0.1025\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 1.7477 - accuracy: 0.3767 - val_loss: 2.8330 - val_accuracy: 0.1055\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.7649 - accuracy: 0.3664 - val_loss: 2.7645 - val_accuracy: 0.1035\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 1.7453 - accuracy: 0.3779 - val_loss: 2.7891 - val_accuracy: 0.0940\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.7332 - accuracy: 0.3816 - val_loss: 2.8757 - val_accuracy: 0.1055\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.7346 - accuracy: 0.3761 - val_loss: 2.7778 - val_accuracy: 0.0915\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.7155 - accuracy: 0.3893 - val_loss: 2.8250 - val_accuracy: 0.1005\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 1.7045 - accuracy: 0.3936 - val_loss: 2.8692 - val_accuracy: 0.0965\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.6842 - accuracy: 0.4006 - val_loss: 2.8792 - val_accuracy: 0.1010\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 1.6734 - accuracy: 0.4008 - val_loss: 2.8741 - val_accuracy: 0.1040\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.6647 - accuracy: 0.4091 - val_loss: 2.8985 - val_accuracy: 0.1015\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.6474 - accuracy: 0.4109 - val_loss: 2.9230 - val_accuracy: 0.1045\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.6367 - accuracy: 0.4155 - val_loss: 2.9243 - val_accuracy: 0.1025\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.6280 - accuracy: 0.4178 - val_loss: 2.9003 - val_accuracy: 0.1020\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.6106 - accuracy: 0.4289 - val_loss: 2.9670 - val_accuracy: 0.1000\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 1.6096 - accuracy: 0.4245 - val_loss: 2.9852 - val_accuracy: 0.0940\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 1.5935 - accuracy: 0.4392 - val_loss: 2.9182 - val_accuracy: 0.0990\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.5828 - accuracy: 0.4423 - val_loss: 3.0757 - val_accuracy: 0.0970\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.5922 - accuracy: 0.4302 - val_loss: 2.9758 - val_accuracy: 0.1070\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 1.5722 - accuracy: 0.4414 - val_loss: 2.9983 - val_accuracy: 0.1075\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.5692 - accuracy: 0.4521 - val_loss: 3.0767 - val_accuracy: 0.0975\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.5620 - accuracy: 0.4509 - val_loss: 2.9809 - val_accuracy: 0.1010\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.5452 - accuracy: 0.4530 - val_loss: 3.1256 - val_accuracy: 0.1025\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.5282 - accuracy: 0.4575 - val_loss: 3.0731 - val_accuracy: 0.1035\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.5278 - accuracy: 0.4556 - val_loss: 3.0644 - val_accuracy: 0.1045\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5232 - accuracy: 0.4563 - val_loss: 3.1701 - val_accuracy: 0.1020\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5077 - accuracy: 0.4626 - val_loss: 3.0943 - val_accuracy: 0.1020\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.4937 - accuracy: 0.4744 - val_loss: 3.1701 - val_accuracy: 0.0960\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.4864 - accuracy: 0.4654 - val_loss: 3.1448 - val_accuracy: 0.1085\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.4746 - accuracy: 0.4843 - val_loss: 3.2222 - val_accuracy: 0.0985\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.4910 - accuracy: 0.4627 - val_loss: 3.1272 - val_accuracy: 0.0985\n",
      "Epoch 117/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 80ms/step - loss: 1.4716 - accuracy: 0.4784 - val_loss: 3.2976 - val_accuracy: 0.1085\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.4832 - accuracy: 0.4661 - val_loss: 3.1558 - val_accuracy: 0.1035\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.4491 - accuracy: 0.4905 - val_loss: 3.1822 - val_accuracy: 0.0990\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.4436 - accuracy: 0.4900 - val_loss: 3.2971 - val_accuracy: 0.1025\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.4309 - accuracy: 0.4924 - val_loss: 3.2326 - val_accuracy: 0.1075\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 1.4377 - accuracy: 0.4884 - val_loss: 3.2834 - val_accuracy: 0.0990\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.4268 - accuracy: 0.4970 - val_loss: 3.3025 - val_accuracy: 0.1065\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.4041 - accuracy: 0.5081 - val_loss: 3.3048 - val_accuracy: 0.1045\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.4150 - accuracy: 0.5042 - val_loss: 3.3680 - val_accuracy: 0.0960\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.4171 - accuracy: 0.4956 - val_loss: 3.2620 - val_accuracy: 0.1015\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.3901 - accuracy: 0.5054 - val_loss: 3.4390 - val_accuracy: 0.1065\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.4014 - accuracy: 0.5001 - val_loss: 3.3563 - val_accuracy: 0.1035\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.3785 - accuracy: 0.5100 - val_loss: 3.3425 - val_accuracy: 0.1030\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.3421 - accuracy: 0.5326 - val_loss: 3.4195 - val_accuracy: 0.1045\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 1.3382 - accuracy: 0.5222 - val_loss: 3.4283 - val_accuracy: 0.0995\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.3533 - accuracy: 0.5170 - val_loss: 3.5302 - val_accuracy: 0.1055\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 1.3323 - accuracy: 0.5241 - val_loss: 3.3774 - val_accuracy: 0.1065\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.3186 - accuracy: 0.5365 - val_loss: 3.4917 - val_accuracy: 0.0990\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 1.3104 - accuracy: 0.5393 - val_loss: 3.4557 - val_accuracy: 0.1015\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.3014 - accuracy: 0.5434 - val_loss: 3.6406 - val_accuracy: 0.0985\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.2822 - accuracy: 0.5497 - val_loss: 3.4781 - val_accuracy: 0.1010\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.2691 - accuracy: 0.5574 - val_loss: 3.5890 - val_accuracy: 0.0990\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.2709 - accuracy: 0.5575 - val_loss: 3.5340 - val_accuracy: 0.1020\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.2569 - accuracy: 0.5614 - val_loss: 3.5986 - val_accuracy: 0.1015\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.2560 - accuracy: 0.5567 - val_loss: 3.6407 - val_accuracy: 0.1010\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.2360 - accuracy: 0.5684 - val_loss: 3.5715 - val_accuracy: 0.0980\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.2235 - accuracy: 0.5685 - val_loss: 3.6531 - val_accuracy: 0.1025\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.2181 - accuracy: 0.5750 - val_loss: 3.6575 - val_accuracy: 0.1020\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.2243 - accuracy: 0.5730 - val_loss: 3.7129 - val_accuracy: 0.1005\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.1954 - accuracy: 0.5851 - val_loss: 3.6777 - val_accuracy: 0.1010\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 1.2151 - accuracy: 0.5773 - val_loss: 3.6427 - val_accuracy: 0.1060\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.1979 - accuracy: 0.5904 - val_loss: 3.8390 - val_accuracy: 0.0970\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.2339 - accuracy: 0.5656 - val_loss: 3.6571 - val_accuracy: 0.0975\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.1872 - accuracy: 0.5815 - val_loss: 3.7840 - val_accuracy: 0.1005\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.1991 - accuracy: 0.5766 - val_loss: 3.8266 - val_accuracy: 0.0935\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.2005 - accuracy: 0.5732 - val_loss: 3.7249 - val_accuracy: 0.1015\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 1.1959 - accuracy: 0.5771 - val_loss: 3.9297 - val_accuracy: 0.1065\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.2001 - accuracy: 0.5779 - val_loss: 3.7919 - val_accuracy: 0.0955\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.1808 - accuracy: 0.5798 - val_loss: 3.9088 - val_accuracy: 0.1020\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 1.1851 - accuracy: 0.5779 - val_loss: 3.7534 - val_accuracy: 0.0970\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 1.1403 - accuracy: 0.6054 - val_loss: 3.8800 - val_accuracy: 0.1010\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 1.1305 - accuracy: 0.6006 - val_loss: 3.9652 - val_accuracy: 0.0990\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 1.1279 - accuracy: 0.6040 - val_loss: 3.9585 - val_accuracy: 0.0920\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.1147 - accuracy: 0.6094 - val_loss: 3.9488 - val_accuracy: 0.0975\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.1190 - accuracy: 0.6037 - val_loss: 3.9377 - val_accuracy: 0.1010\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.0982 - accuracy: 0.6146 - val_loss: 4.0615 - val_accuracy: 0.0985\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.0935 - accuracy: 0.6106 - val_loss: 4.0196 - val_accuracy: 0.0965\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.0855 - accuracy: 0.6200 - val_loss: 4.0326 - val_accuracy: 0.1010\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.0651 - accuracy: 0.6233 - val_loss: 4.0780 - val_accuracy: 0.0980\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.0627 - accuracy: 0.6280 - val_loss: 4.0716 - val_accuracy: 0.1010\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.0496 - accuracy: 0.6345 - val_loss: 4.0698 - val_accuracy: 0.1000\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.0456 - accuracy: 0.6395 - val_loss: 4.1602 - val_accuracy: 0.1010\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.0464 - accuracy: 0.6299 - val_loss: 4.1058 - val_accuracy: 0.0985\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.0319 - accuracy: 0.6449 - val_loss: 4.2405 - val_accuracy: 0.1015\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.0451 - accuracy: 0.6267 - val_loss: 4.0848 - val_accuracy: 0.1005\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.0450 - accuracy: 0.6316 - val_loss: 4.2416 - val_accuracy: 0.0995\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.0333 - accuracy: 0.6384 - val_loss: 4.1716 - val_accuracy: 0.1045\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.0127 - accuracy: 0.6449 - val_loss: 4.1251 - val_accuracy: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.0022 - accuracy: 0.6571 - val_loss: 4.4254 - val_accuracy: 0.1040\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.0208 - accuracy: 0.6399 - val_loss: 4.1659 - val_accuracy: 0.1025\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.0112 - accuracy: 0.6439 - val_loss: 4.1931 - val_accuracy: 0.1035\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.9844 - accuracy: 0.6607 - val_loss: 4.3938 - val_accuracy: 0.1010\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.9844 - accuracy: 0.6529 - val_loss: 4.2481 - val_accuracy: 0.1065\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.9777 - accuracy: 0.6570 - val_loss: 4.4113 - val_accuracy: 0.1020\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.9850 - accuracy: 0.6578 - val_loss: 4.3635 - val_accuracy: 0.0980\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.9669 - accuracy: 0.6616 - val_loss: 4.3837 - val_accuracy: 0.1025\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.9414 - accuracy: 0.6762 - val_loss: 4.4685 - val_accuracy: 0.1010\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.9319 - accuracy: 0.6745 - val_loss: 4.4998 - val_accuracy: 0.0990\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.9354 - accuracy: 0.6747 - val_loss: 4.4386 - val_accuracy: 0.1005\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.9227 - accuracy: 0.6855 - val_loss: 4.4934 - val_accuracy: 0.1030\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.9271 - accuracy: 0.6741 - val_loss: 4.4928 - val_accuracy: 0.1005\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.9199 - accuracy: 0.6798 - val_loss: 4.6017 - val_accuracy: 0.0970\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.9097 - accuracy: 0.6815 - val_loss: 4.5023 - val_accuracy: 0.0995\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.9033 - accuracy: 0.6930 - val_loss: 4.6506 - val_accuracy: 0.1035\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.9039 - accuracy: 0.6840 - val_loss: 4.5439 - val_accuracy: 0.0990\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.8854 - accuracy: 0.6967 - val_loss: 4.5940 - val_accuracy: 0.0960\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.8876 - accuracy: 0.6894 - val_loss: 4.7370 - val_accuracy: 0.0945\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.8811 - accuracy: 0.6981 - val_loss: 4.6128 - val_accuracy: 0.0975\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.8738 - accuracy: 0.6984 - val_loss: 4.7676 - val_accuracy: 0.1010\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.8901 - accuracy: 0.6884 - val_loss: 4.5735 - val_accuracy: 0.1025\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.8780 - accuracy: 0.6996 - val_loss: 4.7662 - val_accuracy: 0.0955\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.8889 - accuracy: 0.6860 - val_loss: 4.7815 - val_accuracy: 0.0970\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.8827 - accuracy: 0.6955 - val_loss: 4.6829 - val_accuracy: 0.1005\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.8572 - accuracy: 0.7051 - val_loss: 4.9626 - val_accuracy: 0.1040\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters:\n",
    "learning_rate = 0.003\n",
    "epochs = 500\n",
    "batch_size = 4000\n",
    "validation_split = 0.2\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, hist = train_model(my_model, x_test_normalized, y_train, epochs, batch_size, validation_split)\n",
    "\n",
    "list_of_metrics_to_plot = [\"accuracy\"]\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Artificial Intelligence",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
